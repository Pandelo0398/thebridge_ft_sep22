{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos las imágenes de los dígitos asi como el conjunto de train y test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "60.000 imagenes de 28x28 pixeles\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_train[0], cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel representa una escala de grises que va del 0 al 255. Siendo 0 el blanco y 255 negro.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5019607843137255"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "255/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13066062"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117648,\n",
       "        0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215687, 0.93333334, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.9843137 , 0.3647059 , 0.32156864,\n",
       "        0.32156864, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7764706 ,\n",
       "        0.7137255 , 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3137255 , 0.6117647 ,\n",
       "        0.41960785, 0.99215686, 0.99215686, 0.8039216 , 0.04313726,\n",
       "        0.        , 0.16862746, 0.6039216 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509807, 0.99215686, 0.74509805, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313726, 0.74509805, 0.99215686, 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13725491, 0.94509804, 0.88235295,\n",
       "        0.627451  , 0.42352942, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764707, 0.9411765 ,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.3647059 , 0.9882353 , 0.99215686, 0.73333335,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.9764706 , 0.99215686, 0.9764706 ,\n",
       "        0.2509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980395, 0.7176471 , 0.99215686, 0.99215686, 0.8117647 ,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.5803922 , 0.8980392 ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.7137255 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882354,\n",
       "        0.8352941 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.7764706 , 0.31764707, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058825, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.7647059 , 0.3137255 ,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568628,\n",
       "        0.6745098 , 0.8862745 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156866, 0.04313726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333336,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137256, 0.5294118 ,\n",
       "        0.5176471 , 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Comprobamos la normalización\n",
    "'''\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "# Capa entrada\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 300,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 100,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Capa salida\n",
    "model.add(keras.layers.Dense(units = 10,\n",
    "                            activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra manera de declarar la red neuronal\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(units = 300, activation='relu'),\n",
    "    keras.layers.Dense(units = 100, activation='relu'),\n",
    "    keras.layers.Dense(units = 10, activation='softmax')\n",
    "]\n",
    "\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.reshaping.flatten.Flatten object at 0x00000210C600ACA0>\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300*784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.SGD(),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente\n",
    "model.compile(\n",
    "    optimizer = \"sgd\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235500"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300*28*28 + 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El batch_size es la cantidad de muestras que utiliza el SGD, y las epochs son las iteraciones que realiza en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390.625"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 1.2426 - accuracy: 0.6871 - val_loss: 0.6042 - val_accuracy: 0.8656\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.5201 - accuracy: 0.8671 - val_loss: 0.3991 - val_accuracy: 0.8973\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4008 - accuracy: 0.8912 - val_loss: 0.3376 - val_accuracy: 0.9084\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.3517 - accuracy: 0.9025 - val_loss: 0.3080 - val_accuracy: 0.9138\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3227 - accuracy: 0.9085 - val_loss: 0.2873 - val_accuracy: 0.9204\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.3015 - accuracy: 0.9144 - val_loss: 0.2702 - val_accuracy: 0.9252\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.2849 - accuracy: 0.9196 - val_loss: 0.2571 - val_accuracy: 0.9259\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2709 - accuracy: 0.9241 - val_loss: 0.2459 - val_accuracy: 0.9303\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 0.2584 - accuracy: 0.9272 - val_loss: 0.2363 - val_accuracy: 0.9323\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2477 - accuracy: 0.9304 - val_loss: 0.2273 - val_accuracy: 0.9356\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.2375 - accuracy: 0.9332 - val_loss: 0.2215 - val_accuracy: 0.9378\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2286 - accuracy: 0.9357 - val_loss: 0.2135 - val_accuracy: 0.9407\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 0.2203 - accuracy: 0.9372 - val_loss: 0.2057 - val_accuracy: 0.9435\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.2125 - accuracy: 0.9395 - val_loss: 0.1992 - val_accuracy: 0.9452\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.2049 - accuracy: 0.9417 - val_loss: 0.1943 - val_accuracy: 0.9458\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1982 - accuracy: 0.9435 - val_loss: 0.1885 - val_accuracy: 0.9487\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1917 - accuracy: 0.9454 - val_loss: 0.1820 - val_accuracy: 0.9502\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1855 - accuracy: 0.9470 - val_loss: 0.1780 - val_accuracy: 0.9504\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1798 - accuracy: 0.9487 - val_loss: 0.1739 - val_accuracy: 0.9530\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1744 - accuracy: 0.9498 - val_loss: 0.1680 - val_accuracy: 0.9547\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1693 - accuracy: 0.9507 - val_loss: 0.1647 - val_accuracy: 0.9555\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1645 - accuracy: 0.9526 - val_loss: 0.1621 - val_accuracy: 0.9566\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1597 - accuracy: 0.9539 - val_loss: 0.1572 - val_accuracy: 0.9573\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1553 - accuracy: 0.9550 - val_loss: 0.1534 - val_accuracy: 0.9588\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1510 - accuracy: 0.9568 - val_loss: 0.1505 - val_accuracy: 0.9600\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.1470 - accuracy: 0.9581 - val_loss: 0.1481 - val_accuracy: 0.9593\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1431 - accuracy: 0.9589 - val_loss: 0.1442 - val_accuracy: 0.9607\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1395 - accuracy: 0.9601 - val_loss: 0.1422 - val_accuracy: 0.9609\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.1356 - accuracy: 0.9615 - val_loss: 0.1408 - val_accuracy: 0.9619\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.1324 - accuracy: 0.9622 - val_loss: 0.1374 - val_accuracy: 0.9622\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1291 - accuracy: 0.9638 - val_loss: 0.1359 - val_accuracy: 0.9632\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1261 - accuracy: 0.9645 - val_loss: 0.1317 - val_accuracy: 0.9647\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1230 - accuracy: 0.9654 - val_loss: 0.1307 - val_accuracy: 0.9651\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1203 - accuracy: 0.9659 - val_loss: 0.1276 - val_accuracy: 0.9660\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1175 - accuracy: 0.9668 - val_loss: 0.1260 - val_accuracy: 0.9663\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1148 - accuracy: 0.9678 - val_loss: 0.1250 - val_accuracy: 0.9667\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1123 - accuracy: 0.9685 - val_loss: 0.1232 - val_accuracy: 0.9663\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1098 - accuracy: 0.9693 - val_loss: 0.1203 - val_accuracy: 0.9668\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1073 - accuracy: 0.9698 - val_loss: 0.1196 - val_accuracy: 0.9676\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1050 - accuracy: 0.9706 - val_loss: 0.1176 - val_accuracy: 0.9679\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1029 - accuracy: 0.9715 - val_loss: 0.1170 - val_accuracy: 0.9680\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1006 - accuracy: 0.9722 - val_loss: 0.1162 - val_accuracy: 0.9684\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.0986 - accuracy: 0.9728 - val_loss: 0.1143 - val_accuracy: 0.9683\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.0966 - accuracy: 0.9735 - val_loss: 0.1136 - val_accuracy: 0.9693\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.0945 - accuracy: 0.9743 - val_loss: 0.1118 - val_accuracy: 0.9695\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.0927 - accuracy: 0.9745 - val_loss: 0.1106 - val_accuracy: 0.9700\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.0908 - accuracy: 0.9751 - val_loss: 0.1099 - val_accuracy: 0.9697\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.0891 - accuracy: 0.9754 - val_loss: 0.1077 - val_accuracy: 0.9714\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.0872 - accuracy: 0.9756 - val_loss: 0.1070 - val_accuracy: 0.9705\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.0856 - accuracy: 0.9765 - val_loss: 0.1058 - val_accuracy: 0.9701\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 128,\n",
    "    epochs = 50,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.0853 - accuracy: 0.9766 - val_loss: 0.1055 - val_accuracy: 0.9701\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.0819 - accuracy: 0.9775 - val_loss: 0.1031 - val_accuracy: 0.9719\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.0789 - accuracy: 0.9782 - val_loss: 0.1005 - val_accuracy: 0.9729\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.0761 - accuracy: 0.9795 - val_loss: 0.0981 - val_accuracy: 0.9729\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.0735 - accuracy: 0.9798 - val_loss: 0.1014 - val_accuracy: 0.9728\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.0710 - accuracy: 0.9811 - val_loss: 0.0990 - val_accuracy: 0.9710\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.0684 - accuracy: 0.9820 - val_loss: 0.0946 - val_accuracy: 0.9736\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.0663 - accuracy: 0.9821 - val_loss: 0.0947 - val_accuracy: 0.9724\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.0639 - accuracy: 0.9832 - val_loss: 0.0925 - val_accuracy: 0.9743\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.0618 - accuracy: 0.9838 - val_loss: 0.0916 - val_accuracy: 0.9749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x210c6429730>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs = 10,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': 1, 'epochs': 50, 'steps': 391}\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [1.2426437139511108,\n",
       "  0.5200545787811279,\n",
       "  0.400776207447052,\n",
       "  0.351667195558548,\n",
       "  0.322683721780777,\n",
       "  0.30145806074142456,\n",
       "  0.2848649024963379,\n",
       "  0.27087387442588806,\n",
       "  0.2583926022052765,\n",
       "  0.2477473020553589,\n",
       "  0.2374938428401947,\n",
       "  0.22862939536571503,\n",
       "  0.22026246786117554,\n",
       "  0.2124628722667694,\n",
       "  0.20494255423545837,\n",
       "  0.19821642339229584,\n",
       "  0.19172562658786774,\n",
       "  0.1854967325925827,\n",
       "  0.1798168122768402,\n",
       "  0.17435187101364136,\n",
       "  0.1693473607301712,\n",
       "  0.16445927321910858,\n",
       "  0.1597159057855606,\n",
       "  0.15529721975326538,\n",
       "  0.15104949474334717,\n",
       "  0.14698822796344757,\n",
       "  0.1430978626012802,\n",
       "  0.13948309421539307,\n",
       "  0.13561686873435974,\n",
       "  0.132430300116539,\n",
       "  0.12910626828670502,\n",
       "  0.1260683834552765,\n",
       "  0.12303562462329865,\n",
       "  0.12027179449796677,\n",
       "  0.11746642738580704,\n",
       "  0.11482097953557968,\n",
       "  0.11229098588228226,\n",
       "  0.1098226010799408,\n",
       "  0.10728510469198227,\n",
       "  0.10504615306854248,\n",
       "  0.10289116203784943,\n",
       "  0.10055843740701675,\n",
       "  0.09855418652296066,\n",
       "  0.09664276987314224,\n",
       "  0.09448003768920898,\n",
       "  0.09267851710319519,\n",
       "  0.09083826839923859,\n",
       "  0.08911091089248657,\n",
       "  0.08720170706510544,\n",
       "  0.0855783075094223],\n",
       " 'accuracy': [0.6870800256729126,\n",
       "  0.8671200275421143,\n",
       "  0.8911600112915039,\n",
       "  0.9025400280952454,\n",
       "  0.9085000157356262,\n",
       "  0.9144200086593628,\n",
       "  0.9195799827575684,\n",
       "  0.9240599870681763,\n",
       "  0.9272000193595886,\n",
       "  0.930400013923645,\n",
       "  0.9331600069999695,\n",
       "  0.9357399940490723,\n",
       "  0.9372199773788452,\n",
       "  0.9394800066947937,\n",
       "  0.9416800141334534,\n",
       "  0.9435399770736694,\n",
       "  0.9453799724578857,\n",
       "  0.9470400214195251,\n",
       "  0.9487199783325195,\n",
       "  0.9498000144958496,\n",
       "  0.9507399797439575,\n",
       "  0.9526200294494629,\n",
       "  0.9539200067520142,\n",
       "  0.9550399780273438,\n",
       "  0.9568399786949158,\n",
       "  0.958079993724823,\n",
       "  0.9589200019836426,\n",
       "  0.9600600004196167,\n",
       "  0.9615399837493896,\n",
       "  0.9622200131416321,\n",
       "  0.9637600183486938,\n",
       "  0.9645400047302246,\n",
       "  0.9654200077056885,\n",
       "  0.9659199714660645,\n",
       "  0.966759979724884,\n",
       "  0.9677799940109253,\n",
       "  0.9685400128364563,\n",
       "  0.9693400263786316,\n",
       "  0.9697999954223633,\n",
       "  0.9706400036811829,\n",
       "  0.9714599847793579,\n",
       "  0.9722200036048889,\n",
       "  0.972819983959198,\n",
       "  0.9735000133514404,\n",
       "  0.9742599725723267,\n",
       "  0.9745399951934814,\n",
       "  0.975059986114502,\n",
       "  0.9754199981689453,\n",
       "  0.9756199717521667,\n",
       "  0.9765200018882751],\n",
       " 'val_loss': [0.6041735410690308,\n",
       "  0.39912232756614685,\n",
       "  0.3375861346721649,\n",
       "  0.3080328106880188,\n",
       "  0.28732213377952576,\n",
       "  0.27022069692611694,\n",
       "  0.2571181058883667,\n",
       "  0.2459203153848648,\n",
       "  0.2363283932209015,\n",
       "  0.22734910249710083,\n",
       "  0.22154684364795685,\n",
       "  0.21353869140148163,\n",
       "  0.20572882890701294,\n",
       "  0.1992034912109375,\n",
       "  0.19434866309165955,\n",
       "  0.18854181468486786,\n",
       "  0.18196699023246765,\n",
       "  0.17795680463314056,\n",
       "  0.17391353845596313,\n",
       "  0.1679774671792984,\n",
       "  0.16473856568336487,\n",
       "  0.16206049919128418,\n",
       "  0.1572336107492447,\n",
       "  0.15338274836540222,\n",
       "  0.1504923403263092,\n",
       "  0.1480754166841507,\n",
       "  0.14424632489681244,\n",
       "  0.1421578824520111,\n",
       "  0.140787735581398,\n",
       "  0.13739368319511414,\n",
       "  0.1358632892370224,\n",
       "  0.13165147602558136,\n",
       "  0.1307130604982376,\n",
       "  0.12760955095291138,\n",
       "  0.12595055997371674,\n",
       "  0.12504200637340546,\n",
       "  0.1232379674911499,\n",
       "  0.1202952116727829,\n",
       "  0.11959634721279144,\n",
       "  0.1175609827041626,\n",
       "  0.11700179427862167,\n",
       "  0.11622854322195053,\n",
       "  0.11432249844074249,\n",
       "  0.11358986794948578,\n",
       "  0.11179902404546738,\n",
       "  0.11061705648899078,\n",
       "  0.109889455139637,\n",
       "  0.10768554359674454,\n",
       "  0.10695596784353256,\n",
       "  0.10579119622707367],\n",
       " 'val_accuracy': [0.8655999898910522,\n",
       "  0.8973000049591064,\n",
       "  0.9083999991416931,\n",
       "  0.9138000011444092,\n",
       "  0.9204000234603882,\n",
       "  0.9251999855041504,\n",
       "  0.9258999824523926,\n",
       "  0.9302999973297119,\n",
       "  0.9322999715805054,\n",
       "  0.9355999827384949,\n",
       "  0.9377999901771545,\n",
       "  0.9406999945640564,\n",
       "  0.9434999823570251,\n",
       "  0.9452000260353088,\n",
       "  0.9458000063896179,\n",
       "  0.9487000107765198,\n",
       "  0.9502000212669373,\n",
       "  0.9503999948501587,\n",
       "  0.953000009059906,\n",
       "  0.9546999931335449,\n",
       "  0.9555000066757202,\n",
       "  0.95660001039505,\n",
       "  0.9573000073432922,\n",
       "  0.9588000178337097,\n",
       "  0.9599999785423279,\n",
       "  0.9592999815940857,\n",
       "  0.9606999754905701,\n",
       "  0.9609000086784363,\n",
       "  0.961899995803833,\n",
       "  0.9621999859809875,\n",
       "  0.9631999731063843,\n",
       "  0.9646999835968018,\n",
       "  0.9650999903678894,\n",
       "  0.9660000205039978,\n",
       "  0.9663000106811523,\n",
       "  0.96670001745224,\n",
       "  0.9663000106811523,\n",
       "  0.9667999744415283,\n",
       "  0.9675999879837036,\n",
       "  0.9678999781608582,\n",
       "  0.9679999947547913,\n",
       "  0.9684000015258789,\n",
       "  0.9682999849319458,\n",
       "  0.9692999720573425,\n",
       "  0.9695000052452087,\n",
       "  0.9700000286102295,\n",
       "  0.9696999788284302,\n",
       "  0.9714000225067139,\n",
       "  0.9704999923706055,\n",
       "  0.9700999855995178]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(history.params)\n",
    "print(history.epoch)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.2426437139511108,\n",
       "  0.5200545787811279,\n",
       "  0.400776207447052,\n",
       "  0.351667195558548,\n",
       "  0.322683721780777,\n",
       "  0.30145806074142456,\n",
       "  0.2848649024963379,\n",
       "  0.27087387442588806,\n",
       "  0.2583926022052765,\n",
       "  0.2477473020553589,\n",
       "  0.2374938428401947,\n",
       "  0.22862939536571503,\n",
       "  0.22026246786117554,\n",
       "  0.2124628722667694,\n",
       "  0.20494255423545837,\n",
       "  0.19821642339229584,\n",
       "  0.19172562658786774,\n",
       "  0.1854967325925827,\n",
       "  0.1798168122768402,\n",
       "  0.17435187101364136,\n",
       "  0.1693473607301712,\n",
       "  0.16445927321910858,\n",
       "  0.1597159057855606,\n",
       "  0.15529721975326538,\n",
       "  0.15104949474334717,\n",
       "  0.14698822796344757,\n",
       "  0.1430978626012802,\n",
       "  0.13948309421539307,\n",
       "  0.13561686873435974,\n",
       "  0.132430300116539,\n",
       "  0.12910626828670502,\n",
       "  0.1260683834552765,\n",
       "  0.12303562462329865,\n",
       "  0.12027179449796677,\n",
       "  0.11746642738580704,\n",
       "  0.11482097953557968,\n",
       "  0.11229098588228226,\n",
       "  0.1098226010799408,\n",
       "  0.10728510469198227,\n",
       "  0.10504615306854248,\n",
       "  0.10289116203784943,\n",
       "  0.10055843740701675,\n",
       "  0.09855418652296066,\n",
       "  0.09664276987314224,\n",
       "  0.09448003768920898,\n",
       "  0.09267851710319519,\n",
       "  0.09083826839923859,\n",
       "  0.08911091089248657,\n",
       "  0.08720170706510544,\n",
       "  0.0855783075094223],\n",
       " 'accuracy': [0.6870800256729126,\n",
       "  0.8671200275421143,\n",
       "  0.8911600112915039,\n",
       "  0.9025400280952454,\n",
       "  0.9085000157356262,\n",
       "  0.9144200086593628,\n",
       "  0.9195799827575684,\n",
       "  0.9240599870681763,\n",
       "  0.9272000193595886,\n",
       "  0.930400013923645,\n",
       "  0.9331600069999695,\n",
       "  0.9357399940490723,\n",
       "  0.9372199773788452,\n",
       "  0.9394800066947937,\n",
       "  0.9416800141334534,\n",
       "  0.9435399770736694,\n",
       "  0.9453799724578857,\n",
       "  0.9470400214195251,\n",
       "  0.9487199783325195,\n",
       "  0.9498000144958496,\n",
       "  0.9507399797439575,\n",
       "  0.9526200294494629,\n",
       "  0.9539200067520142,\n",
       "  0.9550399780273438,\n",
       "  0.9568399786949158,\n",
       "  0.958079993724823,\n",
       "  0.9589200019836426,\n",
       "  0.9600600004196167,\n",
       "  0.9615399837493896,\n",
       "  0.9622200131416321,\n",
       "  0.9637600183486938,\n",
       "  0.9645400047302246,\n",
       "  0.9654200077056885,\n",
       "  0.9659199714660645,\n",
       "  0.966759979724884,\n",
       "  0.9677799940109253,\n",
       "  0.9685400128364563,\n",
       "  0.9693400263786316,\n",
       "  0.9697999954223633,\n",
       "  0.9706400036811829,\n",
       "  0.9714599847793579,\n",
       "  0.9722200036048889,\n",
       "  0.972819983959198,\n",
       "  0.9735000133514404,\n",
       "  0.9742599725723267,\n",
       "  0.9745399951934814,\n",
       "  0.975059986114502,\n",
       "  0.9754199981689453,\n",
       "  0.9756199717521667,\n",
       "  0.9765200018882751],\n",
       " 'val_loss': [0.6041735410690308,\n",
       "  0.39912232756614685,\n",
       "  0.3375861346721649,\n",
       "  0.3080328106880188,\n",
       "  0.28732213377952576,\n",
       "  0.27022069692611694,\n",
       "  0.2571181058883667,\n",
       "  0.2459203153848648,\n",
       "  0.2363283932209015,\n",
       "  0.22734910249710083,\n",
       "  0.22154684364795685,\n",
       "  0.21353869140148163,\n",
       "  0.20572882890701294,\n",
       "  0.1992034912109375,\n",
       "  0.19434866309165955,\n",
       "  0.18854181468486786,\n",
       "  0.18196699023246765,\n",
       "  0.17795680463314056,\n",
       "  0.17391353845596313,\n",
       "  0.1679774671792984,\n",
       "  0.16473856568336487,\n",
       "  0.16206049919128418,\n",
       "  0.1572336107492447,\n",
       "  0.15338274836540222,\n",
       "  0.1504923403263092,\n",
       "  0.1480754166841507,\n",
       "  0.14424632489681244,\n",
       "  0.1421578824520111,\n",
       "  0.140787735581398,\n",
       "  0.13739368319511414,\n",
       "  0.1358632892370224,\n",
       "  0.13165147602558136,\n",
       "  0.1307130604982376,\n",
       "  0.12760955095291138,\n",
       "  0.12595055997371674,\n",
       "  0.12504200637340546,\n",
       "  0.1232379674911499,\n",
       "  0.1202952116727829,\n",
       "  0.11959634721279144,\n",
       "  0.1175609827041626,\n",
       "  0.11700179427862167,\n",
       "  0.11622854322195053,\n",
       "  0.11432249844074249,\n",
       "  0.11358986794948578,\n",
       "  0.11179902404546738,\n",
       "  0.11061705648899078,\n",
       "  0.109889455139637,\n",
       "  0.10768554359674454,\n",
       "  0.10695596784353256,\n",
       "  0.10579119622707367],\n",
       " 'val_accuracy': [0.8655999898910522,\n",
       "  0.8973000049591064,\n",
       "  0.9083999991416931,\n",
       "  0.9138000011444092,\n",
       "  0.9204000234603882,\n",
       "  0.9251999855041504,\n",
       "  0.9258999824523926,\n",
       "  0.9302999973297119,\n",
       "  0.9322999715805054,\n",
       "  0.9355999827384949,\n",
       "  0.9377999901771545,\n",
       "  0.9406999945640564,\n",
       "  0.9434999823570251,\n",
       "  0.9452000260353088,\n",
       "  0.9458000063896179,\n",
       "  0.9487000107765198,\n",
       "  0.9502000212669373,\n",
       "  0.9503999948501587,\n",
       "  0.953000009059906,\n",
       "  0.9546999931335449,\n",
       "  0.9555000066757202,\n",
       "  0.95660001039505,\n",
       "  0.9573000073432922,\n",
       "  0.9588000178337097,\n",
       "  0.9599999785423279,\n",
       "  0.9592999815940857,\n",
       "  0.9606999754905701,\n",
       "  0.9609000086784363,\n",
       "  0.961899995803833,\n",
       "  0.9621999859809875,\n",
       "  0.9631999731063843,\n",
       "  0.9646999835968018,\n",
       "  0.9650999903678894,\n",
       "  0.9660000205039978,\n",
       "  0.9663000106811523,\n",
       "  0.96670001745224,\n",
       "  0.9663000106811523,\n",
       "  0.9667999744415283,\n",
       "  0.9675999879837036,\n",
       "  0.9678999781608582,\n",
       "  0.9679999947547913,\n",
       "  0.9684000015258789,\n",
       "  0.9682999849319458,\n",
       "  0.9692999720573425,\n",
       "  0.9695000052452087,\n",
       "  0.9700000286102295,\n",
       "  0.9696999788284302,\n",
       "  0.9714000225067139,\n",
       "  0.9704999923706055,\n",
       "  0.9700999855995178]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.242644</td>\n",
       "      <td>0.68708</td>\n",
       "      <td>0.604174</td>\n",
       "      <td>0.8656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.520055</td>\n",
       "      <td>0.86712</td>\n",
       "      <td>0.399122</td>\n",
       "      <td>0.8973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.400776</td>\n",
       "      <td>0.89116</td>\n",
       "      <td>0.337586</td>\n",
       "      <td>0.9084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.351667</td>\n",
       "      <td>0.90254</td>\n",
       "      <td>0.308033</td>\n",
       "      <td>0.9138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.322684</td>\n",
       "      <td>0.90850</td>\n",
       "      <td>0.287322</td>\n",
       "      <td>0.9204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.301458</td>\n",
       "      <td>0.91442</td>\n",
       "      <td>0.270221</td>\n",
       "      <td>0.9252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.284865</td>\n",
       "      <td>0.91958</td>\n",
       "      <td>0.257118</td>\n",
       "      <td>0.9259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.270874</td>\n",
       "      <td>0.92406</td>\n",
       "      <td>0.245920</td>\n",
       "      <td>0.9303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.258393</td>\n",
       "      <td>0.92720</td>\n",
       "      <td>0.236328</td>\n",
       "      <td>0.9323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.247747</td>\n",
       "      <td>0.93040</td>\n",
       "      <td>0.227349</td>\n",
       "      <td>0.9356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.237494</td>\n",
       "      <td>0.93316</td>\n",
       "      <td>0.221547</td>\n",
       "      <td>0.9378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.228629</td>\n",
       "      <td>0.93574</td>\n",
       "      <td>0.213539</td>\n",
       "      <td>0.9407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.220262</td>\n",
       "      <td>0.93722</td>\n",
       "      <td>0.205729</td>\n",
       "      <td>0.9435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.212463</td>\n",
       "      <td>0.93948</td>\n",
       "      <td>0.199203</td>\n",
       "      <td>0.9452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.204943</td>\n",
       "      <td>0.94168</td>\n",
       "      <td>0.194349</td>\n",
       "      <td>0.9458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.198216</td>\n",
       "      <td>0.94354</td>\n",
       "      <td>0.188542</td>\n",
       "      <td>0.9487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.191726</td>\n",
       "      <td>0.94538</td>\n",
       "      <td>0.181967</td>\n",
       "      <td>0.9502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.185497</td>\n",
       "      <td>0.94704</td>\n",
       "      <td>0.177957</td>\n",
       "      <td>0.9504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.179817</td>\n",
       "      <td>0.94872</td>\n",
       "      <td>0.173914</td>\n",
       "      <td>0.9530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.174352</td>\n",
       "      <td>0.94980</td>\n",
       "      <td>0.167977</td>\n",
       "      <td>0.9547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.169347</td>\n",
       "      <td>0.95074</td>\n",
       "      <td>0.164739</td>\n",
       "      <td>0.9555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.164459</td>\n",
       "      <td>0.95262</td>\n",
       "      <td>0.162060</td>\n",
       "      <td>0.9566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.159716</td>\n",
       "      <td>0.95392</td>\n",
       "      <td>0.157234</td>\n",
       "      <td>0.9573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.155297</td>\n",
       "      <td>0.95504</td>\n",
       "      <td>0.153383</td>\n",
       "      <td>0.9588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.151049</td>\n",
       "      <td>0.95684</td>\n",
       "      <td>0.150492</td>\n",
       "      <td>0.9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.146988</td>\n",
       "      <td>0.95808</td>\n",
       "      <td>0.148075</td>\n",
       "      <td>0.9593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.143098</td>\n",
       "      <td>0.95892</td>\n",
       "      <td>0.144246</td>\n",
       "      <td>0.9607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.139483</td>\n",
       "      <td>0.96006</td>\n",
       "      <td>0.142158</td>\n",
       "      <td>0.9609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.135617</td>\n",
       "      <td>0.96154</td>\n",
       "      <td>0.140788</td>\n",
       "      <td>0.9619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.132430</td>\n",
       "      <td>0.96222</td>\n",
       "      <td>0.137394</td>\n",
       "      <td>0.9622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.129106</td>\n",
       "      <td>0.96376</td>\n",
       "      <td>0.135863</td>\n",
       "      <td>0.9632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.126068</td>\n",
       "      <td>0.96454</td>\n",
       "      <td>0.131651</td>\n",
       "      <td>0.9647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.123036</td>\n",
       "      <td>0.96542</td>\n",
       "      <td>0.130713</td>\n",
       "      <td>0.9651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.120272</td>\n",
       "      <td>0.96592</td>\n",
       "      <td>0.127610</td>\n",
       "      <td>0.9660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.117466</td>\n",
       "      <td>0.96676</td>\n",
       "      <td>0.125951</td>\n",
       "      <td>0.9663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.114821</td>\n",
       "      <td>0.96778</td>\n",
       "      <td>0.125042</td>\n",
       "      <td>0.9667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.112291</td>\n",
       "      <td>0.96854</td>\n",
       "      <td>0.123238</td>\n",
       "      <td>0.9663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.109823</td>\n",
       "      <td>0.96934</td>\n",
       "      <td>0.120295</td>\n",
       "      <td>0.9668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.107285</td>\n",
       "      <td>0.96980</td>\n",
       "      <td>0.119596</td>\n",
       "      <td>0.9676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.105046</td>\n",
       "      <td>0.97064</td>\n",
       "      <td>0.117561</td>\n",
       "      <td>0.9679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.102891</td>\n",
       "      <td>0.97146</td>\n",
       "      <td>0.117002</td>\n",
       "      <td>0.9680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.100558</td>\n",
       "      <td>0.97222</td>\n",
       "      <td>0.116229</td>\n",
       "      <td>0.9684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.098554</td>\n",
       "      <td>0.97282</td>\n",
       "      <td>0.114322</td>\n",
       "      <td>0.9683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.096643</td>\n",
       "      <td>0.97350</td>\n",
       "      <td>0.113590</td>\n",
       "      <td>0.9693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.094480</td>\n",
       "      <td>0.97426</td>\n",
       "      <td>0.111799</td>\n",
       "      <td>0.9695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.092679</td>\n",
       "      <td>0.97454</td>\n",
       "      <td>0.110617</td>\n",
       "      <td>0.9700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.090838</td>\n",
       "      <td>0.97506</td>\n",
       "      <td>0.109889</td>\n",
       "      <td>0.9697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.089111</td>\n",
       "      <td>0.97542</td>\n",
       "      <td>0.107686</td>\n",
       "      <td>0.9714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.087202</td>\n",
       "      <td>0.97562</td>\n",
       "      <td>0.106956</td>\n",
       "      <td>0.9705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.085578</td>\n",
       "      <td>0.97652</td>\n",
       "      <td>0.105791</td>\n",
       "      <td>0.9701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   1.242644   0.68708  0.604174        0.8656\n",
       "1   0.520055   0.86712  0.399122        0.8973\n",
       "2   0.400776   0.89116  0.337586        0.9084\n",
       "3   0.351667   0.90254  0.308033        0.9138\n",
       "4   0.322684   0.90850  0.287322        0.9204\n",
       "5   0.301458   0.91442  0.270221        0.9252\n",
       "6   0.284865   0.91958  0.257118        0.9259\n",
       "7   0.270874   0.92406  0.245920        0.9303\n",
       "8   0.258393   0.92720  0.236328        0.9323\n",
       "9   0.247747   0.93040  0.227349        0.9356\n",
       "10  0.237494   0.93316  0.221547        0.9378\n",
       "11  0.228629   0.93574  0.213539        0.9407\n",
       "12  0.220262   0.93722  0.205729        0.9435\n",
       "13  0.212463   0.93948  0.199203        0.9452\n",
       "14  0.204943   0.94168  0.194349        0.9458\n",
       "15  0.198216   0.94354  0.188542        0.9487\n",
       "16  0.191726   0.94538  0.181967        0.9502\n",
       "17  0.185497   0.94704  0.177957        0.9504\n",
       "18  0.179817   0.94872  0.173914        0.9530\n",
       "19  0.174352   0.94980  0.167977        0.9547\n",
       "20  0.169347   0.95074  0.164739        0.9555\n",
       "21  0.164459   0.95262  0.162060        0.9566\n",
       "22  0.159716   0.95392  0.157234        0.9573\n",
       "23  0.155297   0.95504  0.153383        0.9588\n",
       "24  0.151049   0.95684  0.150492        0.9600\n",
       "25  0.146988   0.95808  0.148075        0.9593\n",
       "26  0.143098   0.95892  0.144246        0.9607\n",
       "27  0.139483   0.96006  0.142158        0.9609\n",
       "28  0.135617   0.96154  0.140788        0.9619\n",
       "29  0.132430   0.96222  0.137394        0.9622\n",
       "30  0.129106   0.96376  0.135863        0.9632\n",
       "31  0.126068   0.96454  0.131651        0.9647\n",
       "32  0.123036   0.96542  0.130713        0.9651\n",
       "33  0.120272   0.96592  0.127610        0.9660\n",
       "34  0.117466   0.96676  0.125951        0.9663\n",
       "35  0.114821   0.96778  0.125042        0.9667\n",
       "36  0.112291   0.96854  0.123238        0.9663\n",
       "37  0.109823   0.96934  0.120295        0.9668\n",
       "38  0.107285   0.96980  0.119596        0.9676\n",
       "39  0.105046   0.97064  0.117561        0.9679\n",
       "40  0.102891   0.97146  0.117002        0.9680\n",
       "41  0.100558   0.97222  0.116229        0.9684\n",
       "42  0.098554   0.97282  0.114322        0.9683\n",
       "43  0.096643   0.97350  0.113590        0.9693\n",
       "44  0.094480   0.97426  0.111799        0.9695\n",
       "45  0.092679   0.97454  0.110617        0.9700\n",
       "46  0.090838   0.97506  0.109889        0.9697\n",
       "47  0.089111   0.97542  0.107686        0.9714\n",
       "48  0.087202   0.97562  0.106956        0.9705\n",
       "49  0.085578   0.97652  0.105791        0.9701"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0882 - accuracy: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08821225166320801, 0.9736999869346619]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cogemos el primero\n",
    "plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.32941177, 0.7254902 , 0.62352943, 0.5921569 ,\n",
       "         0.23529412, 0.14117648, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.87058824, 0.99607843, 0.99607843, 0.99607843,\n",
       "         0.99607843, 0.94509804, 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.6666667 , 0.20392157, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.2627451 , 0.44705883, 0.28235295, 0.44705883,\n",
       "         0.6392157 , 0.8901961 , 0.99607843, 0.88235295, 0.99607843,\n",
       "         0.99607843, 0.99607843, 0.98039216, 0.8980392 , 0.99607843,\n",
       "         0.99607843, 0.54901963, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "         0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "         0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.3254902 , 0.99215686,\n",
       "         0.81960785, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.08627451, 0.9137255 , 1.        ,\n",
       "         0.3254902 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5058824 , 0.99607843, 0.93333334,\n",
       "         0.17254902, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.23137255, 0.9764706 , 0.99607843, 0.24313726,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.03529412, 0.8039216 , 0.972549  , 0.22745098, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.49411765, 0.99607843, 0.7137255 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.29411766,\n",
       "         0.9843137 , 0.9411765 , 0.22352941, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.07450981, 0.8666667 ,\n",
       "         0.99607843, 0.6509804 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "         0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.14901961, 0.99607843, 0.99607843,\n",
       "         0.3019608 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.12156863, 0.8784314 , 0.99607843, 0.4509804 ,\n",
       "         0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.23921569, 0.9490196 , 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.8117647 , 0.07058824, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 243ms/step\n",
      "(1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.001, 0.   , 0.   , 0.   , 0.999, 0.   ,\n",
       "        0.   ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test[:1]).round(3)\n",
    "print(predictions.shape)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[2].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#housing.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos datos\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns = housing.feature_names)\n",
    "df['target'] = housing['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362.8125"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11610/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 3s 5ms/step - loss: 0.8715 - val_loss: 0.5150\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5378 - val_loss: 0.4451\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4802 - val_loss: 0.4297\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4604 - val_loss: 0.4011\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4441 - val_loss: 0.3945\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4698 - val_loss: 0.4237\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5391 - val_loss: 0.3862\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4510 - val_loss: 0.3794\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4224 - val_loss: 0.3716\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4153 - val_loss: 0.3684\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4135 - val_loss: 0.3671\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4101 - val_loss: 0.4154\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4083 - val_loss: 0.3627\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4734 - val_loss: 0.3639\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4065 - val_loss: 0.3617\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4005 - val_loss: 0.3542\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3958 - val_loss: 0.3498\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3938 - val_loss: 0.3464\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3922 - val_loss: 0.3500\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3898 - val_loss: 0.3413\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'relu',\n",
    "                      input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mean_squared_error\",\n",
    "             optimizer = \"sgd\")\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs = 20,\n",
    "                   validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*30 + 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 30)                270       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 1s 3ms/step - loss: 0.3988\n",
      "0.3988266587257385\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)\n",
    "print(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 106ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.2437038],\n",
       "       [2.630054 ],\n",
       "       [2.2850068],\n",
       "       [2.800885 ],\n",
       "       [1.7085471]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test[:5])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "Son funciones predefinidas de Keras a aplicar durante el entrenamiento\n",
    "Por ejemplo, `ModelCheckpoint` sirve para que el modelo se vaya guardando tras cada epoch. Así no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 0.3874\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3846\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3839\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3808\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3783\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3761\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3730\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3717\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3697\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3686\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3737\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3673\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3644\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3641\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3668\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3599\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3604\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3603\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3554\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3676\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3673\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3540\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3507\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3573\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3506\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3592\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3485\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3479\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3427\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3434\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.h5\")\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=30,\n",
    "                   callbacks = [checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3407 - val_loss: 0.3047\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3417 - val_loss: 0.3078\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3414 - val_loss: 0.3107\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3374 - val_loss: 0.3090\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3367 - val_loss: 0.3059\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3440 - val_loss: 0.3050\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5)\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=20,\n",
    "                    validation_data = (X_valid, y_valid),\n",
    "                   callbacks = [early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
